""" msg_ingest.py

A single-file, production-ready(ish) Python module for ingesting Outlook .msg emails and converting them into structured, RAG-ready facts with provenance. It handles:

• Parsing headers/body/attachments from .msg files • Reconstructing inline images referenced via CID (content-id) inside HTML • Separating the latest human-written reply from quoted history • Lifting HTML tables to DataFrames and turning them into short factual atoms • OCR of screenshot/image attachments (basic denoise + threshold) • Lightweight entity extraction (amounts, percentages, durations, policy ids, dates) • Emission of small, citeable "evidence atoms" suitable for vector/sparse indexing

All steps include detailed comments and docstrings so the module can be understood and adapted quickly. External heavy dependencies are avoided; optional notes are called out inline.

Prerequisites (pip): pip install extract-msg beautifulsoup4 lxml pandas python-dateutil pytz 
email-reply-parser pillow pytesseract opencv-python spacy rapidfuzz

System dependency (for OCR): • Tesseract OCR binary must be installed and accessible on PATH. On Windows it is commonly at: C:\Program Files\Tesseract-OCR\tesseract.exe

Usage (CLI-ish): python msg_ingest.py "C:/path/to/email.msg"

Programmatic usage: from msg_ingest import process_msg_file result = process_msg_file("C:/path/to/email.msg") # 'result' is a dict; serialize to JSON or index its atoms in your RAG store. """ from future import annotations

──────────────────────────────────────────────────────────────────────────────

Standard library imports

──────────────────────────────────────────────────────────────────────────────

import base64  # convert bytes → base64 for data URLs import io      # in-memory bytes buffers for PIL import json    # optional: return/print JSON import os      # paths and environment import re      # regex for lightweight entity extraction import sys     # CLI entry point import uuid    # stable identifiers for sources/attachments from dataclasses import dataclass, asdict  # not heavily used but handy from typing import Any, Dict, List, Optional, Tuple

──────────────────────────────────────────────────────────────────────────────

Third‑party imports (install per the header above)

──────────────────────────────────────────────────────────────────────────────

import extract_msg  # core .msg parser from bs4 import BeautifulSoup  # robust HTML processing import pandas as pd  # tables, CSV-like operations from email_reply_parser import EmailReplyParser  # thread segmentation from dateutil import parser as dtparse  # forgiving datetime parsing from dateutil import tz  # timezone normalization from rapidfuzz import fuzz  # placeholder if you want fuzzy matching in future

from PIL import Image  # image handling for OCR import pytesseract  # OCR binding to tesseract import cv2  # OpenCV for basic denoise/threshold import numpy as np  # arrays for OpenCV/PIL interop

──────────────────────────────────────────────────────────────────────────────

Constants and configuration

──────────────────────────────────────────────────────────────────────────────

CID_PREFIX = "cid:"  # prefix used in HTML <img src="cid:..."> TZ_UTC = tz.gettz("UTC")  # normalize email dates to UTC ISO 8601

If Tesseract is in a non-default location, uncomment and set the path:

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

Simple signature/footer hints (expand for your org)

SIG_HINTS = ( "Regards", "Thanks", "Best,", "Sent from", "Disclaimer", "Confidentiality", "This email" )

──────────────────────────────────────────────────────────────────────────────

Utility helpers

──────────────────────────────────────────────────────────────────────────────

def normalize_date(datestr: Optional[str]) -> Optional[str]: """Parse an email date string and return an ISO 8601 UTC timestamp.

We intentionally swallow parsing errors and return None so the pipeline
is resilient to malformed headers found in old/outlook-exported messages.
"""
if not datestr:
    return None  # nothing to do
try:
    return dtparse.parse(datestr).astimezone(TZ_UTC).isoformat()  # ISO UTC
except Exception:
    return None  # keep going even if the header is odd

def guess_image_mime(filename: Optional[str]) -> Optional[str]: """Guess an image MIME type from a filename extension.

This is only used for constructing data URLs when replacing CID images.
"""
if not filename:
    return None
ext = os.path.splitext(filename)[1].lower()
return {
    ".png": "image/png",
    ".jpg": "image/jpeg",
    ".jpeg": "image/jpeg",
    ".gif": "image/gif",
    ".bmp": "image/bmp",
    ".tif": "image/tiff",
    ".tiff": "image/tiff",
}.get(ext)

def html_to_text(html: str) -> str: """Convert HTML to plain text using BeautifulSoup while dropping scripts/styles.""" if not html: return "" soup = BeautifulSoup(html, "lxml")  # fast, lenient parser for tag in soup(["script", "style"]): tag.decompose()  # remove non-content noise return soup.get_text("\n", strip=True)  # preserve line boundaries

──────────────────────────────────────────────────────────────────────────────

.msg parsing and inline image reconstruction

──────────────────────────────────────────────────────────────────────────────

def load_msg(path: str) -> Dict[str, Any]: """Load a .msg file via extract_msg and normalize its raw parts.

Returns a dict so we can easily serialize/pass around without custom classes.
"""
m = extract_msg.Message(path)  # parse the .msg file
m.convertRtf = True  # allow the library to convert RTF body if present

# Build a normalized skeleton with headers/body/html/attachments
raw: Dict[str, Any] = {
    "source_id": f"msg_{uuid.uuid4().hex}",  # unique ID for provenance
    "path": os.path.abspath(path),  # absolute disk path for audit
    "headers": {
        "from": m.sender or (m.headers.get("from") if m.headers else None),
        "to": m.to,  # semicolon-separated string in extract_msg
        "cc": m.cc,  # same as 'to'
        "subject": m.subject,
        "date": normalize_date(m.date),  # ISO timestamp or None
        "message_id": getattr(m, "messageId", None) or (m.headers.get("message-id") if m.headers else None),
        "in_reply_to": (m.headers.get("in-reply-to") if m.headers else None),
        "conversation_topic": (m.headers.get("conversation-topic") if m.headers else None) or m.subject,
    },
    "text_body": m.body or "",  # plaintext body if available
    "html_body": m.htmlBody or "",  # HTML body if available
    "attachments": [],  # list of dicts we will fill below
}

# Normalize attachments (including inline ones)
for a in m.attachments:
    att = {
        "filename": getattr(a, "longFilename", None) or getattr(a, "shortFilename", None) or f"att_{uuid.uuid4().hex}",
        "content_id": getattr(a, "contentId", None),  # can be used for CID mapping
        "mimetype": getattr(a, "mimeType", None) or getattr(a, "mimetype", None),
        "data": a.data,  # raw bytes
    }
    raw["attachments"].append(att)

return raw

def replace_cid_with_data_urls(html: str, attachments: List[Dict[str, Any]]) -> str: """Replace <img src="cid:..."> references with actual data URLs.

Outlook often embeds images via content-id (CID). For downstream model/renderer
compatibility, we convert those into data URLs (data:image/*;base64,...) so the
HTML becomes self-contained and previewable.
"""
if not html:
    return html  # nothing to do

# Build a map from CID → data URL for quick replacement
cid_map: Dict[str, str] = {}
for a in attachments:
    cid = (a.get("content_id") or "").strip("<>")  # normalize <id> → id
    if not cid:
        continue  # skip non-inline attachments
    mimetype = a.get("mimetype") or guess_image_mime(a.get("filename"))
    if mimetype and a.get("data"):
        b64 = base64.b64encode(a["data"]).decode("utf-8")
        cid_map[cid] = f"data:{mimetype};base64,{b64}"

soup = BeautifulSoup(html, "lxml")  # parse once
for img in soup.find_all("img"):
    src = img.get("src")
    if src and src.lower().startswith(CID_PREFIX):  # img refers to cid:...
        key = src[len(CID_PREFIX):].strip("<>")
        if key in cid_map:
            img["src"] = cid_map[key]  # swap in self-contained data URL

return str(soup)

──────────────────────────────────────────────────────────────────────────────

Thread splitting (newest human message vs quoted history)

──────────────────────────────────────────────────────────────────────────────

def split_email_thread(text_body: str) -> Dict[str, Any]: """Split the email body into fragments and identify the most recent message.

We prefer the latest non-quoted fragment as the user's "new content".
Fallback: if parsing fails, return the whole body as new content.
"""
parsed = EmailReplyParser.read(text_body or "")  # robust segmentation
fragments = parsed.fragments  # list of Fragment objects

segments: List[Dict[str, Any]] = []
for i, f in enumerate(fragments):
    segments.append({
        "index": i,
        "is_quoted": f.quoted,      # True if this is part of quoted history
        "is_signature": f.signature, # True if it looks like a signature block
        "content": (f.content or "").strip()  # the actual text
    })

# Pick the first non-empty, non-quoted fragment as the new content
new_content = next((s["content"] for s in segments if s["content"] and not s["is_quoted"]), (text_body or "").strip())

return {"segments": segments, "new_content": new_content}

──────────────────────────────────────────────────────────────────────────────

HTML table extraction → DataFrames → short factual atoms

──────────────────────────────────────────────────────────────────────────────

def table_to_facts(df: pd.DataFrame, source_id: str, table_index: int) -> List[Dict[str, Any]]: """Convert a DataFrame into row-wise, header-aware short fact strings.

Example output atom: "Waiting Period: 24 months; Condition: Cataract; Notes: NA"
"""
facts: List[Dict[str, Any]] = []

# Clean trivial all-null rows/cols up-front to reduce noise
df = df.dropna(how="all", axis=0).dropna(how="all", axis=1)

for r_idx, row in df.iterrows():
    pieces: List[str] = []
    for col, val in row.items():
        if pd.isna(val):
            continue  # skip empty cells
        label = str(col).strip()
        value = str(val).strip()
        pieces.append(f"{label}: {value}")  # header-aware key: value
    if pieces:
        facts.append({
            "atom": "; ".join(pieces),  # compact sentence-like string
            "modality": "table",
            "source_id": source_id,
            "table_index": int(table_index),
            "row_index": int(r_idx),
        })

return facts

def html_tables_to_dfs_and_facts(html: str, source_id: str) -> Tuple[List[pd.DataFrame], List[Dict[str, Any]]]: """Find <table> tags in HTML, parse them to DataFrames, and emit row facts.""" if not html: return [], []

dfs: List[pd.DataFrame] = []
facts: List[Dict[str, Any]] = []

try:
    tables = pd.read_html(html)  # uses lxml under the hood
    for idx, df in enumerate(tables):
        dfs.append(df)
        facts.extend(table_to_facts(df, source_id=source_id, table_index=idx))
except ValueError:
    # No tables found; that's fine.
    pass

return dfs, facts

──────────────────────────────────────────────────────────────────────────────

OCR for screenshots / photo attachments (basic, fast path)

──────────────────────────────────────────────────────────────────────────────

def ocr_image_bytes(b: bytes) -> Tuple[str, bool]: """OCR an image byte-string using Tesseract with light preprocessing.

Returns (text, looks_like_table) where the boolean is a coarse heuristic that
allows you to escalate to a heavier table extractor or VLM when True.
"""
img = Image.open(io.BytesIO(b)).convert("L")  # grayscale for stable OCR

# Convert PIL → OpenCV (expects BGR); then denoise & threshold
arr = np.array(img)
blur = cv2.medianBlur(arr, 3)  # small blur to reduce salt/pepper noise
th = cv2.adaptiveThreshold(
    blur, 255,
    cv2.ADAPTIVE_THRESH_MEAN_C,
    cv2.THRESH_BINARY,
    35, 11
)

# OCR using tesseract; you can pass language hints via lang="eng"
text = pytesseract.image_to_string(th)

# Heuristic: presence of pipes/tabs or repeated multi-space columns → table-ish
looks_like_table = text.count("|") + text.count("\t") > 5 or bool(re.search(r"\s{2,}\S+\s{2,}", text))

return text.strip(), looks_like_table

def extract_text_from_images(attachments: List[Dict[str, Any]]) -> List[Dict[str, Any]]: """OCR image/* attachments; return list of blocks with text and flags.""" outputs: List[Dict[str, Any]] = []

for i, a in enumerate(attachments):
    mt = (a.get("mimetype") or "").lower()
    fn = (a.get("filename") or "").lower()

    # Consider common image types either by MIME or filename extension
    is_image = "image" in mt or fn.endswith((".png", ".jpg", ".jpeg", ".gif", ".bmp", ".tif", ".tiff"))
    if not is_image:
        continue

    img_bytes = a.get("data")
    if not img_bytes:
        continue  # skip empty attachment records

    try:
        text, maybe_table = ocr_image_bytes(img_bytes)
        outputs.append({
            "attachment_index": i,
            "filename": a.get("filename"),
            "content_id": a.get("content_id"),
            "ocr_text": text,
            "looks_like_table": maybe_table,
        })
    except Exception as e:
        # OCR failures shouldn't break the ingest; record minimal info.
        outputs.append({
            "attachment_index": i,
            "filename": a.get("filename"),
            "content_id": a.get("content_id"),
            "ocr_error": str(e),
        })

return outputs

──────────────────────────────────────────────────────────────────────────────

Lightweight entity extraction for compliance flavors

──────────────────────────────────────────────────────────────────────────────

MONEY_RE = re.compile(r"(?:₹|INR|Rs.?|INR.)\s?[\d,]+(?:.\d+)?", re.I) PERCENT_RE = re.compile(r"\b\d{1,3}(?:.\d+)?\s?%\b", re.I) DURATION_RE = re.compile(r"\b\d{1,3}\s?(?:day|days|month|months|year|years)\b", re.I) POLICY_ID_RE = re.compile(r"\b[A-Z]{2,5}-?\d{3,8}\b") DATE_FUZZY_RE = re.compile(r"\b\d{1,2}-/ [-/ ]\d{2,4}\b", re.I)

def extract_entities_simple(text: str) -> Dict[str, List[str]]: """Return a dict of simple entity lists using regexes (fast and explainable).""" return { "money": list({m.group(0) for m in MONEY_RE.finditer(text)}), "percent": list({m.group(0) for m in PERCENT_RE.finditer(text)}), "duration": list({m.group(0) for m in DURATION_RE.finditer(text)}), "policy_ids": list({m.group(0) for m in POLICY_ID_RE.finditer(text)}), "dates": list({m.group(0) for m in DATE_FUZZY_RE.finditer(text)}), }

──────────────────────────────────────────────────────────────────────────────

Evidence atoms (short, citeable facts) from free text

──────────────────────────────────────────────────────────────────────────────

def atoms_from_text_block(text: str, source_id: str, section: str) -> List[Dict[str, Any]]: """Split a text block into sentence-like atoms while keeping provenance.""" atoms: List[Dict[str, Any]] = []

# Conservative sentence splitter: punctuation + space; avoids heavy NLP deps
for sent in re.split(r"(?<=[.!?])\s+", (text or "").strip()):
    if not sent:
        continue
    atoms.append({
        "atom": sent,
        "modality": "text",
        "source_id": source_id,
        "section": section,
    })

return atoms

──────────────────────────────────────────────────────────────────────────────

Orchestration: everything → one normalized JSON object

──────────────────────────────────────────────────────────────────────────────

def strip_signature(text: str) -> str: """Heuristic removal of common email signature/footers from the latest reply.""" lines = (text or "").splitlines() cut = len(lines) for i, line in enumerate(lines): if any(hint in line for hint in SIG_HINTS) and i > 2:  # keep short greetings cut = i break return "\n".join(lines[:cut]).strip()

def process_msg_file(path: str) -> Dict[str, Any]: """Full pipeline for a .msg file → structured, RAG-ready dict.

Steps:
  1) Parse .msg and normalize headers/body/attachments
  2) Rebuild inline images (CID) inside HTML as data URLs
  3) Split thread and isolate newest human-written content
  4) Extract HTML tables → DataFrames → row facts
  5) OCR any image attachments (screenshots/photos)
  6) Aggregate entities from newest text + OCR text
  7) Build evidence atoms (short, citeable facts) with provenance
"""
raw = load_msg(path)  # step 1
source_id = raw["source_id"]

# step 2: convert inline CID images to data URLs so HTML is self-contained
html = replace_cid_with_data_urls(raw["html_body"], raw["attachments"]) if raw.get("html_body") else ""

# step 3: choose a text body to segment (prefer plaintext; else HTML→text)
text_for_thread = raw.get("text_body") or html_to_text(html)
thread = split_email_thread(text_for_thread)

# clean the latest new message of obvious signature/footer noise
new_text = strip_signature(thread["new_content"]) if thread.get("new_content") else ""

# step 4: extract tables from HTML (if any) and create factual atoms
html_tables, table_facts = html_tables_to_dfs_and_facts(html, source_id)

# step 5: OCR image attachments (attachments list already includes inline ones)
ocr_blocks = extract_text_from_images(raw["attachments"]) if raw.get("attachments") else []

# step 6: entities from the concatenation of new_text + OCR text
combined_for_entities = (new_text or "") + "\n\n" + "\n\n".join(
    b.get("ocr_text", "") for b in ocr_blocks if b.get("ocr_text")
)
entities = extract_entities_simple(combined_for_entities)

# step 7: build atoms from new text and OCR text, then append table facts
atoms: List[Dict[str, Any]] = []
atoms += atoms_from_text_block(new_text, source_id, section="email:new_content")
for b in ocr_blocks:
    if b.get("ocr_text"):
        atoms += atoms_from_text_block(
            b["ocr_text"], source_id, section=f"attachment:{b.get('filename')}:ocr"
        )
atoms += table_facts  # pre-built atoms from tables

# Construct the final normalized payload (JSON-serializable)
result: Dict[str, Any] = {
    "type": "email",
    "source_id": source_id,
    "provenance": {"path": raw["path"]},
    "headers": raw["headers"],
    "new_message_text": new_text,
    "thread_segments": thread["segments"],  # segmented for audit
    "html_with_data_urls": html,  # for UI render/debugging
    "tables_preview": [df.head(3).to_dict(orient="split") for df in html_tables],
    "ocr_blocks": ocr_blocks,
    "entities": entities,
    "atoms": atoms,
}

return result

──────────────────────────────────────────────────────────────────────────────

CLI entry point for quick manual testing

──────────────────────────────────────────────────────────────────────────────

if name == "main": if len(sys.argv) < 2: print("Usage: python msg_ingest.py <path-to-email.msg>") sys.exit(1)

msg_path = sys.argv[1]
if not os.path.isfile(msg_path):
    print(f"File not found: {msg_path}")
    sys.exit(1)

try:
    out = process_msg_file(msg_path)
    # Print a compact preview so terminals aren't flooded
    preview = json.dumps(out, indent=2)  # pretty JSON
    # Limit output length in console; you can redirect to a file when needed
    print(preview[:4000])
except Exception as e:
    print(f"Error processing {msg_path}: {e}")
    sys.exit(2)

