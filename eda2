Excellent â€” based on User Story 3 (Attachment-based AI Response Proof of Concept), Iâ€™ve broken it down into clear, logically sequenced subtasks, aligned to your Jira structure and keeping in mind how analytics teams typically execute such PoCs.

Each subtask below includes title, goal, and short description, ready to paste directly into Jira.


---

ðŸ§© User Story 3 Subtasks â€“ Attachment-based AI Response PoC


---

Sub-Task 3.1.1 â€“ Define PoC Scope, Dataset, and Evaluation Metrics

Goal: Establish a clear experimental design and baseline.
Description:
Identify and document the PoC scope, including number of sample ASK queries, attachment types to be covered, and expected output format. Define the evaluation framework â€” metrics to measure AI uplift (e.g., relevance score, factual accuracy, completeness). Prepare a small, diverse dataset of ASK queries with corresponding mock attachments representing real-world compliance scenarios.


---

Sub-Task 3.1.2 â€“ Attachment Data Collection and Preparation

Goal: Prepare representative sample attachments.
Description:
Create or curate a set of attachments covering different file formats such as .msg, .pdf, .docx, .xlsx, .png, .jpg, and others. Ensure attachments mimic realistic ASK query scenarios (e.g., policy documents, regulatory notices, screenshots, email threads). Annotate and organize these attachments in a local storage structure for controlled testing.


---

Sub-Task 3.1.3 â€“ Attachment Parsing and Content Extraction

Goal: Build local extraction pipeline to derive meaningful text and metadata.
Description:
Develop Python-based scripts or notebooks to extract relevant content from each attachment type using appropriate techniques:

.msg â†’ Extract subject, sender, recipients, timestamps, and email body.

.pdf / .docx â†’ Use text parsers (e.g., pdfplumber, docx2txt) to extract readable text and tables.

.xlsx â†’ Extract cell content and summarize structured data.

.png / .jpg â†’ Apply OCR (e.g., pytesseract) to extract embedded text.
Normalize and clean extracted content, storing outputs in a consistent format (JSON/CSV).



---

Sub-Task 3.1.4 â€“ Attachment Content Summarization and Context Integration

Goal: Enrich user query with attachment-derived context.
Description:
Implement summarization and relevance filtering logic (e.g., using an LLM or keyword-based methods) to condense extracted text into concise, query-relevant snippets. Append or embed these snippets as contextual inputs to the AI model. Ensure integration logic avoids redundant or irrelevant content.


---

Sub-Task 3.1.5 â€“ AI Response Generation (With and Without Attachment Context)

Goal: Generate baseline and context-enriched AI responses.
Description:
Run the Gen-AI model (e.g., GPT or internal LLM) for two experiment tracks per query:

Track A: Using only the query text.

Track B: Using query text + attachment context.
Collect outputs systematically, tagging each response with experiment metadata (query ID, attachment ID, run type).



---

Sub-Task 3.1.6 â€“ Comparative Evaluation and Uplift Analysis

Goal: Quantify improvements in AI performance when attachments are used.
Description:
Evaluate responses using both automated metrics (semantic similarity, response relevance) and manual expert review for a sample subset. Compare uplift across key performance dimensions: accuracy, completeness, relevance, and hallucination reduction. Summarize key findings statistically and visually.


---

Sub-Task 3.1.7 â€“ Documentation and Final Report

Goal: Summarize PoC results, insights, and recommendations.
Description:
Prepare a final PoC report and presentation deck that includes:

Experimental setup and methodology.

Attachment extraction logic and coverage.

Quantitative uplift metrics and qualitative observations.

Key challenges and improvement suggestions.
Store all related scripts, results, and documentation in the teamâ€™s local Git repository.



---

Sub-Task 3.1.8 â€“ Knowledge Transfer and Next Steps

Goal: Ensure findings are shared and actionable for next phase.
Description:
Conduct a walkthrough session with IT and product stakeholders (involved in User Story 2) to share findings, demonstrate PoC outputs, and recommend integration pathways for production deployment. Capture feedback and identify refinement areas for future phases.


---

âœ… Summary of Subtask Flow

Order	Subtask Title	Primary Owner	Output

3.1.1	Define scope & metrics	Analytics Lead	PoC plan & dataset schema
3.1.2	Prepare attachments	Data Analyst	Mock attachments
3.1.3	Extract content	Data Engineer	Parsed text dataset
3.1.4	Integrate context	Data Scientist	Query+context dataset
3.1.5	Generate responses	AI Specialist	Response dataset
3.1.6	Analyze uplift	Analytics Lead	Comparison results
3.1.7	Report findings	Analytics Lead	PoC report & slides
3.1.8	Knowledge transfer	Project Lead	Stakeholder sign-off



---

Would you like me to also create the summary line items for Jira (Epic > US > Subtask hierarchy) in Excel-style format â€” consistent with your screenshot (like 3.1.1, 3.1.2â€¦ under User Story 3)?
I can structure them in a table that matches your template exactly, ready for direct copy-paste.

