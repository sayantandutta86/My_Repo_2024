User Story 3 – of Epic 1: Attachment-based AI Response Proof of Concept

Description:
As an Analytics Team member, I want to perform a proof of concept (PoC) in a local offline analytics environment using sample ASK queries and their associated attachments, so that we can demonstrate the impact of including attachment-derived context on the quality and relevance of AI-driven responses compared to when attachments are not used.

This PoC will simulate end-to-end behavior of the enhanced ADA pipeline, focusing on attachment data understanding, extraction, and integration into query context prior to AI response generation.


---

Scope & Approach:

1. Use a representative set of sample ASK queries and corresponding mock attachments generated locally (not from production).


2. Process and extract meaningful information from various attachment formats, including but not limited to:

.msg – Email content, sender, receiver, subject, message body, and attachments within emails.

.pdf / .docx – Text, tabular content, and key phrases.

.xlsx – Structured data, headers, and summarized numerical insights.

.png / .jpg – Use OCR or vision-based models to extract embedded text or label information.



3. Integrate extracted information into the query context provided to the AI model.


4. Run two experiment tracks for each query:

Track A: Without attachment-derived context.

Track B: With attachment-derived context.



5. Measure and compare uplift across key AI response quality dimensions such as:

Relevance to user query.

Accuracy of interpretation.

Completeness of answer.

Reduction in “no answer” or hallucination cases.





---

Acceptance Criteria (AC):

AC1: PoC pipeline is established locally with capability to process multiple attachment types listed above.

AC2: Attachment content extraction achieves at least a baseline accuracy threshold for each format (as defined in team documentation).

AC3: Comparative experiment results are documented, highlighting measurable uplift in AI-driven response quality when attachment context is included.

AC4: A final report (slides + summary document) is prepared outlining methodology, datasets, observed uplift, challenges, and recommendations for production implementation.

AC5: PoC code, extraction scripts, and configuration are stored in the team’s local Git repository for future reference and audit.



---

Dependencies:

Runs in parallel with User Story 2 (Appian → RCDP attachment transfer) being implemented by the IT team.

Does not depend on RCDP integration; uses locally generated attachments for experimentation.



---

Expected Deliverables:

Processed dataset of sample queries and extracted attachment text.

Comparative AI response quality report with quantitative uplift metrics.

Presentation deck summarizing PoC outcomes and readiness for next phase integration.
