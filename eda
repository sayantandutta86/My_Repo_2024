Okay, here is Python code using the spaCy library to perform Exploratory Data Analysis (EDA) on query text. This code will help you derive insights about the nature of the text data and identify patterns within individual queries.
I've included comments to explain each part of the process. Based on the provided PDF file, I've extracted a few sample queries to demonstrate the code's functionality. You can replace the sample_queries list with your actual data.
# Import the spaCy library
import spacy
from collections import Counter
import pandas as pd # Used for displaying results neatly

# Load the pre-trained English language model from spaCy
# You might need to download it first by running: python -m spacy download en_core_web_sm
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("Downloading language model 'en_core_web_sm' (this may take a few minutes)...")
    spacy.cli.download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

# --- Sample Data ---
# Extracted sample queries based on the provided PDF file (abc_1.pdf)
# Replace this list with your actual list of query strings
sample_queries = [
    "HTC plans to attend the CCF Chinasoft Conference from November 15th to 17th. HTC will have two guest speakers flying from Guangzhou to attend the conference. According to the service content of the organizer CCF, the plane tickets and hotel arrangements for the two speakers will be provided by the organizer. Does this require an FCC case to be submitted? How to submit it specifically?", # 
    "The Engagement is at pre-contract stage in Archer. Refer the attached excel sheet along with IRQ and RRQ pdf extracts for details of the proposed outsourcing arrangement/engagement. No Red Flags or Risk indicators identified during the screening or the risk assessment. However, since the risk rating on the Engagement is High, consulting AB&C compliance for advice at the pre-contract stage.", # 
    "Due to HASE staff's family member passed away, the HASE AMO/Individual HASE staff would like to offer a flower plaque to a HASE staff's family member in relation to grief. Please advise if these scenarios are subject to AB&C requirements and whether this G&E should be in GER system.", # 
    "Request to determine relevant AB&C risk representative and obtain guidance on the issues or concerns identified while performing pre-contract control task in Archer. Refer the attachments for details of the proposed outsourcing arrangement/engagement.", # 
    "For HBCN, where can I find the latest G&E Pre-approval thresholds and guidance? like below JP one. Many thanks." # 
]

# --- EDA Functions ---

def analyze_query(query_text):
    """
    Performs NLP analysis on a single query using spaCy.

    Args:
        query_text (str): The text of the query.

    Returns:
        spacy.tokens.doc.Doc: The processed spaCy Doc object.
    """
    # Process the text with the spaCy pipeline
    doc = nlp(query_text)
    return doc

def get_basic_stats(doc):
    """
    Calculates basic statistics for a processed query.

    Args:
        doc (spacy.tokens.doc.Doc): The processed spaCy Doc object.

    Returns:
        dict: A dictionary containing basic statistics.
    """
    stats = {
        "Total Tokens": len(doc),
        "Non-Stopword Tokens": len([token for token in doc if not token.is_stop and not token.is_punct]),
        "Unique Tokens (Case-Insensitive)": len(set(token.lower_ for token in doc if not token.is_punct)),
        "Sentence Count": len(list(doc.sents))
    }
    return stats

def get_pos_counts(doc):
    """
    Counts the Part-of-Speech (POS) tags in a processed query.

    Args:
        doc (spacy.tokens.doc.Doc): The processed spaCy Doc object.

    Returns:
        collections.Counter: A Counter object with POS tag frequencies.
    """
    # Count POS tags, excluding punctuation
    pos_tags = [token.pos_ for token in doc if not token.is_punct]
    return Counter(pos_tags)

def get_ner_counts(doc):
    """
    Counts the Named Entity Recognition (NER) labels in a processed query.

    Args:
        doc (spacy.tokens.doc.Doc): The processed spaCy Doc object.

    Returns:
        collections.Counter: A Counter object with NER label frequencies.
    """
    ner_labels = [ent.label_ for ent in doc.ents]
    return Counter(ner_labels)

def print_query_insights(query_index, query_text, doc):
    """
    Prints the derived insights for a single query.

    Args:
        query_index (int): The index of the query in the list.
        query_text (str): The original query text.
        doc (spacy.tokens.doc.Doc): The processed spaCy Doc object.
    """
    print(f"\n--- Insights for Query #{query_index + 1} ---")
    # print(f"Original Text: {query_text}") # Uncomment to see the original text

    # Basic Statistics
    stats = get_basic_stats(doc)
    print("\nBasic Statistics:")
    for key, value in stats.items():
        print(f"- {key}: {value}")

    # Part-of-Speech (POS) Tag Counts
    pos_counts = get_pos_counts(doc)
    print("\nPart-of-Speech Counts (Top 5):")
    if pos_counts:
        for tag, count in pos_counts.most_common(5):
            print(f"- {tag}: {count} ({spacy.explain(tag)})")
    else:
        print("- No POS tags found (excluding punctuation).")

    # Named Entity Recognition (NER) Counts
    ner_counts = get_ner_counts(doc)
    print("\nNamed Entity Counts:")
    if ner_counts:
        for label, count in ner_counts.most_common():
            print(f"- {label}: {count} ({spacy.explain(label)})")
    else:
        print("- No named entities found.")

    # Example Tokens and their details (first 10)
    print("\nExample Tokens (First 10):")
    print("Token | Lemma | POS | Tag | Dependency | Is Stopword?")
    print("------|-------|-----|-----|------------|-------------")
    for token in doc[:10]:
        print(f"{token.text:<6}| {token.lemma_:<6}| {token.pos_:<4}| {token.tag_:<4}| {token.dep_:<10}| {token.is_stop}")

    print("-" * 20)

# --- Main Processing Loop ---
all_stats = []
all_pos = Counter()
all_ner = Counter()

print(f"Starting EDA for {len(sample_queries)} queries...")

for i, query in enumerate(sample_queries):
    # Analyze the current query
    processed_doc = analyze_query(query)

    # Print insights for the individual query
    print_query_insights(i, query, processed_doc)

    # Aggregate statistics for overall analysis
    all_stats.append(get_basic_stats(processed_doc))
    all_pos.update(get_pos_counts(processed_doc))
    all_ner.update(get_ner_counts(processed_doc))

# --- Overall Insights ---
print("\n=== Overall Insights Across All Queries ===")

# Average Statistics
if all_stats:
    avg_stats_df = pd.DataFrame(all_stats)
    print("\nAverage Basic Statistics:")
    print(avg_stats_df.mean().round(2)) # Calculate and print the mean for each stat column
else:
    print("\nNo statistics to calculate averages.")

# Overall Top POS Tags
print("\nOverall Top 10 Part-of-Speech Tags:")
if all_pos:
    for tag, count in all_pos.most_common(10):
        print(f"- {tag}: {count} ({spacy.explain(tag)})")
else:
    print("- No POS tags found.")

# Overall Top NER Labels
print("\nOverall Top Named Entity Labels:")
if all_ner:
    for label, count in all_ner.most_common():
        print(f"- {label}: {count} ({spacy.explain(label)})")
else:
    print("- No named entities found.")

print("\nEDA Complete.")

Explanation:
 * Import Libraries: Imports spacy for NLP tasks and Counter for counting items. pandas is used optionally for nicer display of average statistics.
 * Load Language Model: Loads spaCy's small English model (en_core_web_sm). It includes vocabulary, syntax, and entity recognition capabilities. If not found, it attempts to download it.
 * Sample Data: Contains a list of example query strings based on the PDF provided. Remember to replace this with your actual data source.
 * analyze_query(query_text): Takes a query string and processes it using the loaded spaCy model (nlp). This returns a Doc object containing tokens, POS tags, entities, etc.
 * get_basic_stats(doc): Calculates simple metrics like token count, non-stopword token count, unique tokens, and sentence count.
 * get_pos_counts(doc): Counts the occurrences of each Part-of-Speech tag (like Noun, Verb, Adjective) in the Doc.
 * get_ner_counts(doc): Counts the occurrences of each Named Entity type (like Person, Organization, Location, Date) identified in the Doc.
 * print_query_insights(...): Takes the results from the analysis functions and prints them in a formatted way for a single query, including examples of tokens and their attributes.
 * Main Processing Loop:
   * Iterates through each query in the sample_queries list.
   * Calls analyze_query to process the text.
   * Calls print_query_insights to show results for the current query.
   * Aggregates the statistics (all_stats, all_pos, all_ner) across all queries.
 * Overall Insights: After processing all queries, it calculates and prints:
   * Average values for the basic statistics across all queries using pandas for easy calculation.
   * The most common POS tags found across the entire dataset.
   * The most common Named Entities found across the entire dataset.
This script provides a solid foundation for your text EDA. You can further customize it by adding more analyses like frequency distributions of specific words, dependency parse visualization, or sentiment analysis if needed.
